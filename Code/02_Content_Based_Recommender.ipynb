{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">NOTE: This Notebook is replicated as a python file for future ChatBot Enginnering, thus no           need to run the code here. The notebook is for expalaining the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Retrieving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from rake_nltk import Rake\n",
    "import psycopg2 as pg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run sql_table.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def con_cur_to_db(dbname=DBNAME, dict_cur=None):\n",
    "    ''' \n",
    "    Returns both a connection and a cursor object for your database\n",
    "    '''\n",
    "    con = pg2.connect(host=IP_ADDRESS,\n",
    "                  dbname=dbname,\n",
    "                  user=USER,\n",
    "                  password=PASSWORD)\n",
    "    if dict_cur:\n",
    "        cur = con.cursor(cursor_factory=RealDictCursor)\n",
    "    else:\n",
    "        cur = con.cursor()\n",
    "    \n",
    "    return con, cur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_query(query, dbname=DBNAME, dict_cur=None, command=False):\n",
    "    '''\n",
    "    Executes a query directly to a database, without having to create a cursor and connection each time. \n",
    "    '''\n",
    "    con, cur = con_cur_to_db(dbname, dict_cur)\n",
    "    cur.execute(f'{query}')\n",
    "    if not command:\n",
    "        data = cur.fetchall()\n",
    "        col_names = []\n",
    "        for elt in cur.description:\n",
    "            col_names.append(elt[0])\n",
    "        con.close()\n",
    "        return data, col_names\n",
    "    con.commit()\n",
    "    con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT *\n",
    "FROM udemy\n",
    "'''\n",
    "data, column_names = execute_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns=column_names)\n",
    "df['headline'] = df['headline'].fillna('No headlines')\n",
    "df['description'] = df['description'].fillna('No description')\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Convert texts into vectors\n",
    "\n",
    "The text embedding converts text (words or sentences) into a numerical vector, they encode words and sentences in fixed-length dense vectorsto drastically improve the processing of textual data.\n",
    "\n",
    "Universal Embeddings: embeddings that are pre-trained on a large corpus and can be plugged in a variety of downstream task models to automatically improve their performance by incorporating some general word/sentence representations learned on the larger dataset.  \n",
    "\n",
    "Google’s Universal Sentence Encoder, published in early 2018. Their encoder uses a transformer-network that is trained on a variety of data sources and a variety of tasks with the aim of dynamically accommodating a wide variety of natural language understanding tasks. The pre-trained Universal Sentence Encoder is publicly available in [Tensorflow-hub.](https://tfhub.dev/). The model is efficient and result in accurate performance on diverse transfer tasks.\n",
    "\n",
    "\n",
    "![https://www.tensorflow.org/hub/modules/google/universal-sentence-encoder/1](https://cdn-images-1.medium.com/max/1600/1*qACWEt8866AOKEYRb-Y5ig.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We simply load the Universal Sentence Encoder module from tensorflow hub. It’s as simple as that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using /var/folders/mq/2zsplllx4zjg6fgp1tlzc4500000gn/T/tfhub_modules to cache modules.\n"
     ]
    }
   ],
   "source": [
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]\n",
    "embed = hub.Module(module_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function `get_features` to wrap tensorflow call. We just create a session and run the embed node in the graph. This gives us the vector for each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(texts):\n",
    "    if type(texts) is str:\n",
    "        texts = [texts]\n",
    "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "        return sess.run(embed(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_words(raw_text):       \n",
    "    letters_only = re.sub(\"[^a-zA-Z0-9.#+]\", \" \", raw_text)\n",
    "    words = letters_only.lower().split()\n",
    "    stops = set(stopwords.words('english'))\n",
    "    extra_stops = set(['learn','course'])\n",
    "    nostop_words = [w for w in words if not w in stops]\n",
    "    meaningful_words = [w for w in nostop_words if not w in extra_stops]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lem_words = [lemmatizer.lemmatize(w) for w in meaningful_words]\n",
    "    return(\" \".join(lem_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text should be preprocessed to remove noises from the texts.\n",
    "\n",
    "We are going to build a Vectors on `objective_summary` in each course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['objectives_summary'] = [clean_words(i) for i in df['objectives_summary']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "BASE_VECTORS = get_features(df['objectives_summary'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, `BASE_VECTORS` is a 79821 X 512 vector matrix converted from `objevive_summary` text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_subscribers'] = df['num_subscribers'].astype('int64')\n",
    "df['avg_rating_recent'] = df['avg_rating_recent'].astype('float64')\n",
    "norm = (df['avg_rating_recent'] * df['num_subscribers'])/(df['num_subscribers']+1000)\n",
    "scaled_foctor = MinMaxScaler().fit_transform(norm.values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the `num_subscribers` and `avg_rating_recent` to numeric values, and generate the scale factor that is based on value of `ave_rating_recent` multiply by `num_subscribers` and divided by `num_subscribers` with denominator 1000. In this way, the scaler considers both number of subscribers and average ratings that course has, so the higher number of subscribers and ratings will have a higher value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, dataframe, vectors, scale):\n",
    "    query = clean_words(query)\n",
    "    print(\"Extracting features...\")\n",
    "    query_vec = get_features(query)\n",
    "    sim = cosine_similarity(vectors, query_vec)\n",
    "    dataframe['scaled_sim'] = sim*scale\n",
    "    top_5 = df.sort_values('scaled_sim', ascending=False)[:5]\n",
    "    return top_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function that convert the input text as a 512 x 1 vector, and use cosine similarity to find simiarity between two vectors (the input text vector and `BASE_VECTORS`. This is nothing but finding the cosine of angle between two vectors. The formula is direcly taken from dot prduct of vectors:\n",
    "\n",
    "## $$\n",
    "cos(\\theta) = \\frac{A \\cdot B}{\\left\\| A\\right\\| \\left\\| B\\right\\| } = \\frac{A \\cdot B}{\\sqrt{\\sum{A_i^2}} \\cdot \\sqrt{\\sum{B_i^2}}}\n",
    "$$\n",
    "\n",
    "The function will find the most similar `objective_summary` with the search term and return 5 top closet courses matches to the search term and has higher number of subscribers and ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook 02_Recommender.ipynb to script\n",
      "[NbConvertApp] Writing 5364 bytes to 02_Recommender.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script 02_Recommender.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, rename the python file to `Recommender.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The MOOC_BOT Engineering\n",
    "<img src=\"../Image/MOOC_BOT.jpg\" width=\"100\">\n",
    "\n",
    "The python files in this folder can deploy MOOC_BOT. It is a simple bot that answers questions about Udemy online coures. The user can ask about popular courses and ask for the bot to recommend the courses Match your search.\n",
    "\n",
    "One important thing to note with this design is that, the data and processing is all handled in the local system. Even though we use IBM, it is used as an API service and none of the internal data is sent to IBM. This way the entire design can be implemented in your workplace without having to worry about data transfers.\n",
    "\n",
    "The [SlackBot](https://github.com/Sundar0989/Movie_Bot/blob/master/slack/Create_slack_app.ipynb) and IBM [Watson account](https://github.com/Sundar0989/Movie_Bot/blob/master/nlp/IBM_Watson_Conversation_setup.ipynb) are built based on the Sundar0989's Tutorial. The API keys and Authorizations are in `config.py`.\n",
    "\n",
    "### 1. `nlp_command.py`\n",
    "Users can interact with MOOC_BOT via Slack. Once the user post a question, it is passed to the backend system for analysis.\n",
    "\n",
    "**Identify Response conditions in the IBM Watson Platform:**\n",
    "\n",
    "- Intents — What the user is trying to ask or query? \n",
    "- Dialog/Interaction — Provide the appropriate request/response for the user question.\n",
    "\n",
    "> For instance, when the user asks:\"Recommend me Udemy courses\", the Intents are detected as predefined \"recommend_moocs\" Intent. Give the response based on the condition set in the `nlp_command.py`\n",
    "\n",
    "\n",
    "### 2. `slack_command.py`\n",
    "\n",
    " Slack APP API engineering: includes functions that return the Slack output mainly as `attachment` method. \n",
    " \n",
    " ```  \n",
    "      slack_client.api_call(\n",
    "      \"chat.postMessage\",\n",
    "      channel= ' ',\n",
    "      attachments='')   \n",
    " ```\n",
    " \n",
    " \n",
    " ### 3. `Recommender.py`\n",
    " \n",
    "The content based recommender function generated in [HERE](../Code/02_Content_Based_Recommender.ipynp). With the text input in the slack bot the Recommender will give you a top 5 courses related to your search.\n",
    "\n",
    "\n",
    "### 4. `chatbot_functions.py`\n",
    "\n",
    "The dataset that processed to give a result as a Slack message. \n",
    "\n",
    "> For instance, `Top5_Overall` variable is the five courses that most people are subscribed to .\n",
    "\n",
    "### 5. ` main.py`\n",
    "\n",
    "Finally, the `main.py` is putting together all functions together to initiate the bot\n",
    "\n",
    "\n",
    "## Step 7: Initiate Bot\n",
    "\n",
    "Navigate to the folder where the main python script exists and run the code below.\n",
    "\n",
    "```sh\n",
    "python3 main.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
